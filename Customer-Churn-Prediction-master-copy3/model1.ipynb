{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           CustomerId      Surname  CreditScore Geography  Gender  Age  \\\n",
      "RowNumber                                                                \n",
      "1            15634602     Hargrave          619    France  Female   42   \n",
      "3            15619304         Onio          502    France  Female   42   \n",
      "6            15574012          Chu          645     Spain    Male   44   \n",
      "8            15656148       Obinna          376   Germany  Female   29   \n",
      "17           15737452        Romeo          653   Germany    Male   58   \n",
      "23           15699309    Gerasimov          510     Spain  Female   38   \n",
      "31           15589475      Azikiwe          591     Spain  Female   39   \n",
      "36           15794171     Lombardo          475    France  Female   45   \n",
      "42           15738148       Clarke          465    France  Female   51   \n",
      "44           15755196       Lavine          834    France  Female   49   \n",
      "47           15602280       Martin          829   Germany  Female   27   \n",
      "48           15771573      Okagbue          637   Germany  Female   39   \n",
      "54           15702298     Parkhill          655   Germany    Male   41   \n",
      "55           15569590          Yoo          601   Germany    Male   42   \n",
      "59           15623944        T'ien          511     Spain  Female   66   \n",
      "71           15703793   Konovalova          738   Germany    Male   58   \n",
      "82           15663706      Leonard          777    France  Female   32   \n",
      "87           15762418         Gant          750     Spain    Male   22   \n",
      "89           15622897       Sharpe          646    France  Female   46   \n",
      "91           15757535         Heap          647     Spain  Female   44   \n",
      "105          15804919     Dunbabin          670     Spain  Female   65   \n",
      "106          15613854      Mauldon          622     Spain  Female   46   \n",
      "110          15744689        T'ang          479   Germany    Male   35   \n",
      "115          15609618      Fanucci          721   Germany    Male   28   \n",
      "119          15661670     Chidozie          524   Germany  Female   31   \n",
      "126          15627360       Fuller          432    France    Male   42   \n",
      "127          15671137    MacDonald          549    France  Female   52   \n",
      "128          15782688       Piccio          625   Germany    Male   56   \n",
      "139          15594408         Chia          584     Spain  Female   48   \n",
      "140          15640905        Vasin          579     Spain  Female   35   \n",
      "...               ...          ...          ...       ...     ...  ...   \n",
      "9833         15814690  Chukwujekwu          595   Germany  Female   64   \n",
      "9835         15799358      Vincent          516    France  Female   46   \n",
      "9839         15616367        Ricci          581   Germany    Male   39   \n",
      "9853         15718765      Maclean          501     Spain    Male   43   \n",
      "9857         15687329         Hope          763   Germany  Female   32   \n",
      "9859         15619514         Bull          507   Germany    Male   40   \n",
      "9860         15615430        Adams          678   Germany    Male   55   \n",
      "9864         15726179      Ferrari          757   Germany  Female   43   \n",
      "9878         15572182   Onwuamaeze          505   Germany  Female   33   \n",
      "9880         15669414       Pisano          486   Germany    Male   62   \n",
      "9883         15785490        Okeke          771    France    Male   50   \n",
      "9885         15686974    Sergeyeva          751    France  Female   48   \n",
      "9896         15796764        Bruno          684   Germany  Female   56   \n",
      "9899         15746569         Tsui          589    France    Male   38   \n",
      "9906         15784124      Emenike          645   Germany    Male   41   \n",
      "9911         15784042           L?          624    France    Male   55   \n",
      "9918         15755731        Davis          635   Germany    Male   53   \n",
      "9921         15673020        Smith          678    France  Female   49   \n",
      "9925         15578865      Palerma          632   Germany  Female   50   \n",
      "9935         15774586         West          692   Germany  Female   43   \n",
      "9948         15732202   Abramovich          615    France    Male   34   \n",
      "9957         15707861        Nucci          520    France  Female   46   \n",
      "9961         15681026     Lucciano          795   Germany  Female   33   \n",
      "9963         15594612        Flynn          702     Spain    Male   44   \n",
      "9976         15666295        Smith          610   Germany    Male   50   \n",
      "9982         15672754     Burbidge          498   Germany    Male   42   \n",
      "9983         15768163      Griffin          655   Germany  Female   46   \n",
      "9992         15769959  Ajuluchukwu          597    France  Female   53   \n",
      "9998         15584532          Liu          709    France  Female   36   \n",
      "9999         15682355    Sabbatini          772   Germany    Male   42   \n",
      "\n",
      "           Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
      "RowNumber                                                                \n",
      "1               2       0.00              1          1               1   \n",
      "3               8  159660.80              3          1               0   \n",
      "6               8  113755.78              2          1               0   \n",
      "8               4  115046.74              4          1               0   \n",
      "17              1  132602.88              1          1               0   \n",
      "23              4       0.00              1          1               0   \n",
      "31              3       0.00              3          1               0   \n",
      "36              0  134264.04              1          1               0   \n",
      "42              8  122522.32              1          0               0   \n",
      "44              2  131394.56              1          0               0   \n",
      "47              9  112045.67              1          1               1   \n",
      "48              9  137843.80              1          1               1   \n",
      "54              8  125561.97              1          0               0   \n",
      "55              1   98495.72              1          1               0   \n",
      "59              4       0.00              1          1               0   \n",
      "71              2  133745.44              4          1               0   \n",
      "82              2       0.00              1          1               0   \n",
      "87              3  121681.82              1          1               0   \n",
      "89              4       0.00              3          1               0   \n",
      "91              5       0.00              3          1               1   \n",
      "105             1       0.00              1          1               1   \n",
      "106             4  107073.27              2          1               1   \n",
      "110             9   92833.89              1          1               0   \n",
      "115             9  154475.54              2          0               1   \n",
      "119             8  107818.63              1          1               0   \n",
      "126             9  152603.45              1          1               0   \n",
      "127             1       0.00              1          0               1   \n",
      "128             0  148507.24              1          1               0   \n",
      "139             2  213146.20              1          1               0   \n",
      "140             1  129490.36              2          0               1   \n",
      "...           ...        ...            ...        ...             ...   \n",
      "9833            2  105736.32              1          1               1   \n",
      "9835            6   62212.29              1          0               1   \n",
      "9839            1  121523.51              1          0               0   \n",
      "9853            6  104533.24              1          0               0   \n",
      "9857            1  108465.65              2          1               0   \n",
      "9859            3  120105.43              1          1               0   \n",
      "9860            4  129646.91              1          1               1   \n",
      "9864            5  131433.33              2          1               1   \n",
      "9878            3  106506.77              3          1               0   \n",
      "9880            9  118356.89              2          1               0   \n",
      "9883            3  105229.72              1          1               1   \n",
      "9885            4       0.00              1          0               1   \n",
      "9896            3  127585.98              3          1               1   \n",
      "9899            4       0.00              1          1               0   \n",
      "9906            2   93925.30              1          1               0   \n",
      "9911            7  118793.60              1          1               1   \n",
      "9918            8  117005.55              1          0               1   \n",
      "9921            3  204510.94              1          0               1   \n",
      "9925            5  107959.39              1          1               1   \n",
      "9935           10  118588.83              1          1               1   \n",
      "9948            1   83503.11              2          1               1   \n",
      "9957           10   85216.61              1          1               0   \n",
      "9961            9  104552.72              1          1               1   \n",
      "9963            9       0.00              1          0               0   \n",
      "9976            1  113957.01              2          1               0   \n",
      "9982            3  152039.70              1          1               1   \n",
      "9983            7  137145.12              1          1               0   \n",
      "9992            4   88381.21              1          1               0   \n",
      "9998            7       0.00              1          0               1   \n",
      "9999            3   75075.31              2          1               0   \n",
      "\n",
      "           EstimatedSalary  Exited  \\\n",
      "RowNumber                            \n",
      "1                101348.88       1   \n",
      "3                113931.57       1   \n",
      "6                149756.71       1   \n",
      "8                119346.88       1   \n",
      "17                 5097.67       1   \n",
      "23               118913.53       1   \n",
      "31               140469.38       1   \n",
      "36                27822.99       1   \n",
      "42               181297.65       1   \n",
      "44               194365.76       1   \n",
      "47               119708.21       1   \n",
      "48               117622.80       1   \n",
      "54               164040.94       1   \n",
      "55                40014.76       1   \n",
      "59                 1643.11       1   \n",
      "71                28373.86       1   \n",
      "82               136458.19       1   \n",
      "87               128643.35       1   \n",
      "89                93251.42       1   \n",
      "91               174205.22       1   \n",
      "105              177655.68       1   \n",
      "106               30984.59       1   \n",
      "110               99449.86       1   \n",
      "115              101300.94       1   \n",
      "119              199725.39       1   \n",
      "126              110265.24       1   \n",
      "127                8636.05       1   \n",
      "128               46824.08       1   \n",
      "139               75161.25       1   \n",
      "140                8590.83       1   \n",
      "...                    ...     ...   \n",
      "9833              89935.73       1   \n",
      "9835             171681.86       1   \n",
      "9839             161655.55       1   \n",
      "9853              81123.59       1   \n",
      "9857              60552.44       1   \n",
      "9859              92075.01       1   \n",
      "9860             184125.10       1   \n",
      "9864               3497.43       1   \n",
      "9878              45445.78       1   \n",
      "9880             168034.83       1   \n",
      "9883              16281.68       1   \n",
      "9885              30165.06       1   \n",
      "9896              80593.49       1   \n",
      "9899              95483.48       1   \n",
      "9906             123982.14       1   \n",
      "9911              95022.02       1   \n",
      "9918             123646.57       1   \n",
      "9921                738.88       1   \n",
      "9925               6985.34       1   \n",
      "9935             161241.65       1   \n",
      "9948              73124.53       1   \n",
      "9957             117369.52       1   \n",
      "9961             120853.83       1   \n",
      "9963              59207.41       1   \n",
      "9976             196526.55       1   \n",
      "9982              53445.17       1   \n",
      "9983             115146.40       1   \n",
      "9992              69384.71       1   \n",
      "9998              42085.58       1   \n",
      "9999              92888.52       1   \n",
      "\n",
      "                            Reason for exiting company  \n",
      "RowNumber                                               \n",
      "1                High Service Charges/Rate of Interest  \n",
      "3                                  Long Response Times  \n",
      "6                High Service Charges/Rate of Interest  \n",
      "8          Inexperienced Staff / Bad customer service   \n",
      "17                                 Long Response Times  \n",
      "23         Inexperienced Staff / Bad customer service   \n",
      "31                           Excess Documents Required  \n",
      "36               High Service Charges/Rate of Interest  \n",
      "42         Inexperienced Staff / Bad customer service   \n",
      "44         Inexperienced Staff / Bad customer service   \n",
      "47                                 Long Response Times  \n",
      "48                                 Long Response Times  \n",
      "54                                 Long Response Times  \n",
      "55         Inexperienced Staff / Bad customer service   \n",
      "59         Inexperienced Staff / Bad customer service   \n",
      "71                           Excess Documents Required  \n",
      "82               High Service Charges/Rate of Interest  \n",
      "87                                 Long Response Times  \n",
      "89         Inexperienced Staff / Bad customer service   \n",
      "91                                 Long Response Times  \n",
      "105              High Service Charges/Rate of Interest  \n",
      "106              High Service Charges/Rate of Interest  \n",
      "110        Inexperienced Staff / Bad customer service   \n",
      "115                                Long Response Times  \n",
      "119              High Service Charges/Rate of Interest  \n",
      "126                          Excess Documents Required  \n",
      "127                          Excess Documents Required  \n",
      "128                                Long Response Times  \n",
      "139              High Service Charges/Rate of Interest  \n",
      "140        Inexperienced Staff / Bad customer service   \n",
      "...                                                ...  \n",
      "9833                               Long Response Times  \n",
      "9835             High Service Charges/Rate of Interest  \n",
      "9839       Inexperienced Staff / Bad customer service   \n",
      "9853             High Service Charges/Rate of Interest  \n",
      "9857       Inexperienced Staff / Bad customer service   \n",
      "9859             High Service Charges/Rate of Interest  \n",
      "9860       Inexperienced Staff / Bad customer service   \n",
      "9864       Inexperienced Staff / Bad customer service   \n",
      "9878                               Long Response Times  \n",
      "9880                         Excess Documents Required  \n",
      "9883                               Long Response Times  \n",
      "9885             High Service Charges/Rate of Interest  \n",
      "9896                         Excess Documents Required  \n",
      "9899       Inexperienced Staff / Bad customer service   \n",
      "9906                               Long Response Times  \n",
      "9911                         Excess Documents Required  \n",
      "9918                               Long Response Times  \n",
      "9921             High Service Charges/Rate of Interest  \n",
      "9925       Inexperienced Staff / Bad customer service   \n",
      "9935             High Service Charges/Rate of Interest  \n",
      "9948             High Service Charges/Rate of Interest  \n",
      "9957       Inexperienced Staff / Bad customer service   \n",
      "9961       Inexperienced Staff / Bad customer service   \n",
      "9963                         Excess Documents Required  \n",
      "9976             High Service Charges/Rate of Interest  \n",
      "9982             High Service Charges/Rate of Interest  \n",
      "9983       Inexperienced Staff / Bad customer service   \n",
      "9992                               Long Response Times  \n",
      "9998       Inexperienced Staff / Bad customer service   \n",
      "9999                         Excess Documents Required  \n",
      "\n",
      "[2037 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "test = pd.read_csv('testtest.csv')\n",
    "data_re=dataset[dataset['Exited']==1]\n",
    "data_re.set_index('RowNumber',inplace=True)\n",
    "print(data_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_re.to_csv('data_re.csv')\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "X_test=test.iloc[:, 3:13].values\n",
    "\n",
    "y= dataset.iloc[:, 13].values\n",
    "y_test= test.iloc[:, 13].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n",
      "[1 0 1 ... 1 1 0]\n",
      "[[700 'Spain' 'Male' 28 4 0.0 2 0 0 80134.88]\n",
      " [580 'Germany' 'Male' 32 4 10020.01 3 1 0 438773.2]\n",
      " [721 'Spain' 'Female' 37 7 10010.01 1 0 1 107332.2]\n",
      " [428 'France' 'Male' 24 3 0.0 1 0 0 65124.8]\n",
      " [511 'France' 'Female' 43 7 109102.4 1 0 1 831021.3]\n",
      " [502 'France' 'Female' 42 8 159660.8 3 1 0 113931.57]\n",
      " [699 'France' 'Female' 39 1 0.0 2 0 0 93826.63]\n",
      " [750 'Spain' 'Male' 22 3 121681.82 1 1 0 128643.35]]\n",
      "[nan nan nan nan nan nan nan nan]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(y)\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roopashreemunegowda/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/roopashreemunegowda/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder_X_3 = LabelEncoder()#creating label encoder object no. 1 to encode region name(index 1 in features)\n",
    "X_test[:, 1] = labelencoder_X_3.fit_transform(X_test[:, 1])#encoding region from string to just 3 no.s 0,1,2 respectively\n",
    "labelencoder_X_4 = LabelEncoder()\n",
    "X_test[:, 2] = labelencoder_X_4.fit_transform(X_test[:, 2])#encoding Gender from string to just 2 no.s 0,1(male,female) respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roopashreemunegowda/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/roopashreemunegowda/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "onehotencoder2 = OneHotEncoder(categorical_features = [1])\n",
    "X_test= onehotencoder2.fit_transform(X_test).toarray()\n",
    "X_test = X_test[:, 1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train=X\n",
    "y_train=y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential#For building the Neural Network layer by layer\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/roopashreemunegowda/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roopashreemunegowda/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential() #UNCOMMENT if not running from saved model\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))#UNCOMMENT if not running from saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roopashreemunegowda/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))#UNCOMMENT if not running from saved model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roopashreemunegowda/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))#UNCOMMENT if not running from saved model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy')#UNCOMMENT if not running from saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/roopashreemunegowda/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roopashreemunegowda/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "10000/10000 [==============================] - 10s 979us/step - loss: 0.4330\n",
      "Epoch 2/150\n",
      "10000/10000 [==============================] - 9s 918us/step - loss: 0.3881\n",
      "Epoch 3/150\n",
      "10000/10000 [==============================] - 10s 987us/step - loss: 0.3753\n",
      "Epoch 4/150\n",
      "10000/10000 [==============================] - 8s 846us/step - loss: 0.3668\n",
      "Epoch 5/150\n",
      "10000/10000 [==============================] - 8s 838us/step - loss: 0.3634\n",
      "Epoch 6/150\n",
      "10000/10000 [==============================] - 8s 840us/step - loss: 0.3589 0s - l\n",
      "Epoch 7/150\n",
      "10000/10000 [==============================] - 8s 825us/step - loss: 0.3574\n",
      "Epoch 8/150\n",
      "10000/10000 [==============================] - 9s 872us/step - loss: 0.3564\n",
      "Epoch 9/150\n",
      "10000/10000 [==============================] - 9s 866us/step - loss: 0.3547\n",
      "Epoch 10/150\n",
      "10000/10000 [==============================] - 8s 835us/step - loss: 0.3541\n",
      "Epoch 11/150\n",
      "10000/10000 [==============================] - 9s 932us/step - loss: 0.3518\n",
      "Epoch 12/150\n",
      "10000/10000 [==============================] - 9s 946us/step - loss: 0.3516\n",
      "Epoch 13/150\n",
      "10000/10000 [==============================] - 9s 937us/step - loss: 0.3509\n",
      "Epoch 14/150\n",
      "10000/10000 [==============================] - 8s 842us/step - loss: 0.3501\n",
      "Epoch 15/150\n",
      "10000/10000 [==============================] - 8s 842us/step - loss: 0.3499\n",
      "Epoch 16/150\n",
      "10000/10000 [==============================] - 8s 832us/step - loss: 0.3496\n",
      "Epoch 17/150\n",
      "10000/10000 [==============================] - 9s 921us/step - loss: 0.3493\n",
      "Epoch 18/150\n",
      "10000/10000 [==============================] - 10s 956us/step - loss: 0.3470\n",
      "Epoch 19/150\n",
      "10000/10000 [==============================] - 10s 956us/step - loss: 0.3460\n",
      "Epoch 20/150\n",
      "10000/10000 [==============================] - 9s 908us/step - loss: 0.3445\n",
      "Epoch 21/150\n",
      "10000/10000 [==============================] - 8s 836us/step - loss: 0.3436\n",
      "Epoch 22/150\n",
      "10000/10000 [==============================] - 8s 835us/step - loss: 0.3436\n",
      "Epoch 23/150\n",
      "10000/10000 [==============================] - 9s 850us/step - loss: 0.3432\n",
      "Epoch 24/150\n",
      "10000/10000 [==============================] - 8s 849us/step - loss: 0.3432\n",
      "Epoch 25/150\n",
      "10000/10000 [==============================] - 9s 912us/step - loss: 0.3434 0s - loss: 0\n",
      "Epoch 26/150\n",
      "10000/10000 [==============================] - 9s 859us/step - loss: 0.3428\n",
      "Epoch 27/150\n",
      "10000/10000 [==============================] - 9s 882us/step - loss: 0.3435 0s - loss:\n",
      "Epoch 28/150\n",
      "10000/10000 [==============================] - 9s 880us/step - loss: 0.3428\n",
      "Epoch 29/150\n",
      "10000/10000 [==============================] - 9s 855us/step - loss: 0.3423\n",
      "Epoch 30/150\n",
      "10000/10000 [==============================] - 9s 861us/step - loss: 0.3430\n",
      "Epoch 31/150\n",
      "10000/10000 [==============================] - 9s 910us/step - loss: 0.3412\n",
      "Epoch 32/150\n",
      "10000/10000 [==============================] - 9s 926us/step - loss: 0.3414\n",
      "Epoch 33/150\n",
      "10000/10000 [==============================] - 10s 995us/step - loss: 0.3424\n",
      "Epoch 34/150\n",
      "10000/10000 [==============================] - 10s 988us/step - loss: 0.3427\n",
      "Epoch 35/150\n",
      "10000/10000 [==============================] - 9s 877us/step - loss: 0.3421\n",
      "Epoch 36/150\n",
      "10000/10000 [==============================] - 9s 884us/step - loss: 0.3394\n",
      "Epoch 37/150\n",
      "10000/10000 [==============================] - 9s 926us/step - loss: 0.3423\n",
      "Epoch 38/150\n",
      "10000/10000 [==============================] - 9s 895us/step - loss: 0.3415\n",
      "Epoch 39/150\n",
      "10000/10000 [==============================] - 9s 886us/step - loss: 0.3401\n",
      "Epoch 40/150\n",
      "10000/10000 [==============================] - 9s 874us/step - loss: 0.3405 0s - loss: 0\n",
      "Epoch 41/150\n",
      "10000/10000 [==============================] - 9s 946us/step - loss: 0.3416\n",
      "Epoch 42/150\n",
      "10000/10000 [==============================] - 10s 982us/step - loss: 0.3405\n",
      "Epoch 43/150\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.3411\n",
      "Epoch 44/150\n",
      "10000/10000 [==============================] - 10s 987us/step - loss: 0.3409\n",
      "Epoch 45/150\n",
      "10000/10000 [==============================] - 10s 990us/step - loss: 0.3399\n",
      "Epoch 46/150\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.3409\n",
      "Epoch 47/150\n",
      "10000/10000 [==============================] - 10s 993us/step - loss: 0.3406\n",
      "Epoch 48/150\n",
      "10000/10000 [==============================] - 9s 900us/step - loss: 0.3403\n",
      "Epoch 49/150\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.3407\n",
      "Epoch 50/150\n",
      "10000/10000 [==============================] - 10s 964us/step - loss: 0.3400\n",
      "Epoch 51/150\n",
      "10000/10000 [==============================] - 9s 896us/step - loss: 0.3404\n",
      "Epoch 52/150\n",
      "10000/10000 [==============================] - 9s 910us/step - loss: 0.3399\n",
      "Epoch 53/150\n",
      "10000/10000 [==============================] - 9s 895us/step - loss: 0.3398\n",
      "Epoch 54/150\n",
      "10000/10000 [==============================] - 9s 903us/step - loss: 0.3403\n",
      "Epoch 55/150\n",
      "10000/10000 [==============================] - 9s 908us/step - loss: 0.3402\n",
      "Epoch 56/150\n",
      "10000/10000 [==============================] - 10s 971us/step - loss: 0.3399\n",
      "Epoch 57/150\n",
      "10000/10000 [==============================] - 10s 1000us/step - loss: 0.3392\n",
      "Epoch 58/150\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.3407\n",
      "Epoch 59/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3381\n",
      "Epoch 60/150\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.3398\n",
      "Epoch 61/150\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.3397\n",
      "Epoch 62/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3391\n",
      "Epoch 63/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3393\n",
      "Epoch 64/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3389\n",
      "Epoch 65/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3393\n",
      "Epoch 66/150\n",
      "10000/10000 [==============================] - 10s 950us/step - loss: 0.33940s - \n",
      "Epoch 67/150\n",
      "10000/10000 [==============================] - 10s 972us/step - loss: 0.3400\n",
      "Epoch 68/150\n",
      "10000/10000 [==============================] - 10s 951us/step - loss: 0.3384\n",
      "Epoch 69/150\n",
      "10000/10000 [==============================] - 10s 957us/step - loss: 0.3384\n",
      "Epoch 70/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3388\n",
      "Epoch 71/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3389\n",
      "Epoch 72/150\n",
      "10000/10000 [==============================] - 10s 975us/step - loss: 0.3387\n",
      "Epoch 73/150\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.3392\n",
      "Epoch 74/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3386\n",
      "Epoch 75/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3384\n",
      "Epoch 76/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3386: 0s - loss:\n",
      "Epoch 77/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3391\n",
      "Epoch 78/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3383\n",
      "Epoch 79/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3384\n",
      "Epoch 80/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3378\n",
      "Epoch 81/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3387\n",
      "Epoch 82/150\n",
      "10000/10000 [==============================] - 10s 961us/step - loss: 0.3377\n",
      "Epoch 83/150\n",
      "10000/10000 [==============================] - 10s 971us/step - loss: 0.3379\n",
      "Epoch 84/150\n",
      "10000/10000 [==============================] - 10s 951us/step - loss: 0.3386\n",
      "Epoch 85/150\n",
      "10000/10000 [==============================] - 10s 960us/step - loss: 0.3386\n",
      "Epoch 86/150\n",
      "10000/10000 [==============================] - 10s 985us/step - loss: 0.3379\n",
      "Epoch 87/150\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.3384\n",
      "Epoch 88/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3381\n",
      "Epoch 89/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3383\n",
      "Epoch 90/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3383\n",
      "Epoch 91/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3376\n",
      "Epoch 92/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 10s 989us/step - loss: 0.3382\n",
      "Epoch 93/150\n",
      "10000/10000 [==============================] - 9s 943us/step - loss: 0.3386\n",
      "Epoch 94/150\n",
      "10000/10000 [==============================] - 10s 966us/step - loss: 0.33840s - loss: 0.\n",
      "Epoch 95/150\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.3380\n",
      "Epoch 96/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3382\n",
      "Epoch 97/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3387\n",
      "Epoch 98/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3377\n",
      "Epoch 99/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3377\n",
      "Epoch 100/150\n",
      "10000/10000 [==============================] - 10s 964us/step - loss: 0.3383\n",
      "Epoch 101/150\n",
      "10000/10000 [==============================] - 10s 970us/step - loss: 0.3376\n",
      "Epoch 102/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3373\n",
      "Epoch 103/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3374\n",
      "Epoch 104/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3380\n",
      "Epoch 105/150\n",
      "10000/10000 [==============================] - 10s 975us/step - loss: 0.3377\n",
      "Epoch 106/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3377\n",
      "Epoch 107/150\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.3378\n",
      "Epoch 108/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3382\n",
      "Epoch 109/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3370\n",
      "Epoch 110/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3370\n",
      "Epoch 111/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3369\n",
      "Epoch 112/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3385\n",
      "Epoch 113/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3384\n",
      "Epoch 114/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3375\n",
      "Epoch 115/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3386\n",
      "Epoch 116/150\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.3367\n",
      "Epoch 117/150\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.3365\n",
      "Epoch 118/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3371\n",
      "Epoch 119/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3365\n",
      "Epoch 120/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3373\n",
      "Epoch 121/150\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.3371\n",
      "Epoch 122/150\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.3378\n",
      "Epoch 123/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3371\n",
      "Epoch 124/150\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.3374\n",
      "Epoch 125/150\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.3374\n",
      "Epoch 126/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3368\n",
      "Epoch 127/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3378\n",
      "Epoch 128/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3368\n",
      "Epoch 129/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3376\n",
      "Epoch 130/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3369\n",
      "Epoch 131/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3366\n",
      "Epoch 132/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3366\n",
      "Epoch 133/150\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.3380\n",
      "Epoch 134/150\n",
      "10000/10000 [==============================] - 10s 998us/step - loss: 0.3372\n",
      "Epoch 135/150\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.3363\n",
      "Epoch 136/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3377\n",
      "Epoch 137/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3377\n",
      "Epoch 138/150\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.3365\n",
      "Epoch 139/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3368\n",
      "Epoch 140/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3370\n",
      "Epoch 141/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3363\n",
      "Epoch 142/150\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.3368\n",
      "Epoch 143/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3376: 0s - los\n",
      "Epoch 144/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3369\n",
      "Epoch 145/150\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.3362\n",
      "Epoch 146/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3364\n",
      "Epoch 147/150\n",
      "10000/10000 [==============================] - 12s 1ms/step - loss: 0.3359\n",
      "Epoch 148/150\n",
      "10000/10000 [==============================] - 13s 1ms/step - loss: 0.3354\n",
      "Epoch 149/150\n",
      "10000/10000 [==============================] - 11s 1ms/step - loss: 0.3363\n",
      "Epoch 150/150\n",
      "10000/10000 [==============================] - 10s 1ms/step - loss: 0.3366\n"
     ]
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train,batch_size=2,nb_epoch=150)#UNCOMMENT if not running from saved model\n",
    "classifier.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'#UNCOMMENT if not running from saved model\n",
    "#classifier=load_model('my_model.h5') #UNCOMMENT if running from the saved model\n",
    "y_pred = classifier.predict(X_test)\n",
    "#y_pred = (y_pred > 0.5)\n",
    "#y_pred=[1 if i>0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01895946]\n",
      " [0.47274175]\n",
      " [0.19629285]\n",
      " [0.11484629]\n",
      " [0.16979694]\n",
      " [0.9737455 ]\n",
      " [0.06358838]\n",
      " [0.07532185]]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)\n",
    "dff=pd.read_csv(\"testtest.csv\")\n",
    "dff['Exited']=y_pred\n",
    "dff.set_index('RowNumber',inplace=True)\n",
    "dff.sort_values('Exited',ascending=False,inplace=True)\n",
    "dff.to_csv('testtest1.csv') #output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
